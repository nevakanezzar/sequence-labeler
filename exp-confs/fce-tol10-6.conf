[config]
path_train = /home/skasewa/stuff/project/data/exp-data/train/fce/fce-public.train.original.tsv,/cluster/project2/mr/skasewa/models/s2s-fce/corruptions/fce-15-tol10.tsv
multiple = 1.5
path_dev = /home/skasewa/stuff/project/data/exp-data/dev/fce/fce-public.dev.original.tsv
path_test = /home/skasewa/stuff/project/data/exp-data/dev/fce/fce-public.dev.original.tsv:/home/skasewa/stuff/project/data/exp-data/test/fce/fce-public.test.original.tsv:/home/skasewa/stuff/project/data/exp-data/test/conll14-test0/nucle.test0.original.tsv:/home/skasewa/stuff/project/data/exp-data/test/conll14-test1/nucle.test1.original.tsv:/home/skasewa/stuff/project/data/exp-data/test/lang8/lang8-test.tsv
main_label = i
conll_eval = False
lowercase_words = True
lowercase_chars = False
replace_digits = True
min_word_freq = 2
use_singletons = False
allowed_word_length = -1
preload_vectors = /home/skasewa/stuff/project/data/googlenews-vectors/GoogleNews-vectors-negative300.txt
word_embedding_size = 300
char_embedding_size = 50
word_recurrent_size = 200
char_recurrent_size = 200
narrow_layer_size = 50
dropout_input = 0.5
best_model_selector = dev_f05:high
epochs = 50
stop_if_no_improvement_for_epochs = 10
learningrate = 1.0
opt_strategy = adadelta
max_batch_size = 512
save = /home/skasewa/stuff/project/models/downstream/fce-tol10-6.model
load = 
random_seed = 420
crf_on_top = False
char_integration_method = attention
garbage_collection = False
lmcost_gamma = 0.1
lmcost_layer_size = 50
lmcost_max_vocab_size = 10000